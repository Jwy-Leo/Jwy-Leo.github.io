 <b>Wen-Yen Chang</b> completed his master's degree at the department of electrical engineering at National Tsing Hua University, Taiwan in 2020. He was a member of <b>Vision Science Lab (<a href='https://aliensunmin.github.io/lab/info.html' target='_blank'>VSLab</a>)</b> advised by Prof. <b>Min Sun</b> and Dr. <b>Ting-Fan Wu</b>. His research interests are centered on <b>computer vision</b>, <b>machine learning</b>, especially in <b>deep learning</b>. <br> I'm actively looking for permanent positions for Master's graduates. Specifically, seeking <strong>AI Engineer / Data Scientist / Software Engineer</strong> position.<br><br>


## <i class="fa fa-chevron-right"></i> Experiences
<table class="table table-hover">
<tr>
  <td>
<p markdown="1" style='margin: 0'>
<strong>Military Service in Taiwan.</strong>
</p>
  </td>
  <td class='col-md-2' style='text-align:right;'>Sep. 2020 - Dec. 2020</td>
</tr>

<tr>
  <td>
<p markdown="1" style='margin: 0'>
<strong>Machine Learning Research Intern (Consultant)</strong><br>
Kaikutek inc. | Taipei<br>
• Research: fast hand-gesture recognition and few-shot learning.<br>
• Cloud Computing System: Server-less training platform by AWS.<br>
</p>
  </td>
  <td class='col-md-2' style='text-align:right;'>May 2020 - Sep. 2020</td>
</tr>

<tr>
  <td>
<p markdown="1" style='margin: 0'>
<strong>DSP Summer Intern</strong><br>
MediaTek inc. | Hsinchu<br>
• Tools Chains development: 5G NR field tried tools.<br>
• DSP analysis: HRAM replay and analysis<br>
</p>
  </td>
  <td class='col-md-2' style='text-align:right;'>July 2019 - Aug. 2019</td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Education

<table class="table table-hover">
  <tr>
    <td>
        <strong>M.Sc. in Electronic Engineering</strong>
          (4.30/4.30)
        <br>
      National Tsing Hua University | Hsinchu
        <p style='margin-top:-1em;margin-bottom:0em' markdown='1'>
        <br> *[Enhance data selection efficiency with variational auto-encoder for object detection’s active learning](https://etd.lib.nctu.edu.tw/cgi-bin/gs32/hugsweb.cgi?o=dnthucdr&s=id=%22G021060617050%22.&searchmode=basic)*
        <br> Advisors: [Min Sun](https://aliensunmin.github.io/), [Ting-Fan Wu](https://www.linkedin.com/in/tingfan-wu-5359669/)
        </p>
    </td>
    <td class="col-md-2" style='text-align:right;'>Feb. 2018 - June 2020</td>
  </tr>
  <tr>
    <td>
        <strong>B.Sc. in Electronic Engineering</strong>
          (4.14/4.30)
        <br>
      National Chung Cheng University | Chiayi
        <p style='margin-top:-1em;margin-bottom:0em' markdown='1'>
        <br> *[Car tracking and finding service in parkinglot system](https://exhibition.ee.ccu.edu.tw/awards_info.php?teamName=%E6%99%BA%E6%85%A7%E7%9C%BC&year=2017)*
        <br> Advisors: [Ching-Chun Huang](https://www.cs.nycu.edu.tw/members/detail/chingchun)
        </p>
    </td>
    <td class="col-md-2" style='text-align:right;'>Sep. 2014 - Feb. 2018</td>
  </tr>
</table>

## <i class="fa fa-chevron-right"></i> Honors & Awards
<table class="table table-hover">
<tr>
  <td>
  Phi Tau Phi Scholastic Honor
  <br><p style="color:grey;font-size:1.2rem">top 1 master student in the electrical engineering department of NTHU.
  </p>
  </td>
  <td class='col-md-2' style='text-align:right;'>2018 - 2020</td>
</tr>
<tr>
  <td>
  Appier Scholarship
    <br><p style="color:grey;font-size:1.2rem">for outstanding students in their research with top conference papers. (research:<a href='#tr-wenyen2019bias_aware' >ECCV 2018</a>)
</p>
  </td>
  <td class='col-md-2' style='text-align:right;'>2018</td>
</tr>
<tr>
  <td>
  The High Distinction Award (Outstanding Award)
    <br><p style="color:grey;font-size:1.2rem">the <a href="#tr-wenyen2018Cinema-graph">undergraudated project</a> is award to 1st prizes in graduation exhibition of electronic engineering, CCU.
</p>
  </td>
  <td class='col-md-2' style='text-align:right;'>2017</td>
</tr>
<tr>
  <td>
  The Chair Award
    <br><p style="color:grey;font-size:1.2rem">the most potential product for <a href="#tr-wenyen2018Cinema-graph">undergraudated project</a> in the graduation exhibition of electronic engineering in CCU.
</p>
  </td>
  <td class='col-md-2' style='text-align:right;'>2017</td>
</tr>
</table>


<!-- without modify -->
## <i class="fa fa-chevron-right"></i> Publications <a href="https://github.com/"><i class="fa fa-code-fork" aria-hidden="true"></i></a>

<a href="https://scholar.google.com/citations?user=W_SbBOMAAAAJ" class="btn btn-primary" style="padding: 0.3em;">
  <i class="ai ai-google-scholar"></i> Google Scholar
</a>

<h2>2020</h2>
<table class="table table-hover">

<tr id="tr-wenyenchang2020_master_thesis" style="background-color: #ffffd0">
<td class="col-md-3"><a href='https://etd.lib.nctu.edu.tw/cgi-bin/gs32/hugsweb.cgi?o=dnthucdr&s=id=%22G021060617050%22.&searchmode=basic' target='_blank'><img src="images/publications/wenyen_chang_master_thesis.jpg" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em>Enhance data selection efficiency with variational auto-encoder for object detection’s active learning</em><br>
    <strong>Wen-Yen Chang</strong><br>
    Master Thesis<br>
    [1] 
[<a href='javascript:;'
    onclick='$("#abs_wenyenchang2020_master_thesis").toggle()'>abs</a>] [<a href='https://etd.lib.nctu.edu.tw/cgi-bin/gs32/hugsweb.cgi?o=dnthucdr&s=id=%22G021060617050%22.&searchmode=basic' target='_blank'>pdf</a>] <br>
    
<div id="abs_wenyenchang2020_master_thesis" style="text-align: justify; display: none" markdown="1">
We apply pool-based active learning on object detection with surveillance video. The pool-based needs to select one batch of images, which have a budget limit in each selection iteration. Our method utilizes the VAE to enhance the diversity property of the selection strategy. Comparing with uncertainty and diversity selection, our method (hybrid strategy) have robust performance in different environments: Our method relies on uncertainty selection strategy to score image, which is more valuable for labeling. Moreover, we dynamic re-weight the uncertainty score of the image to avoid selecting similar data, which causes the redundant information for object detection model. First, we cluster the latent space of VAE by k-means in order to get similar data pseudo-label. Second, we re-weight uncertainty scores of similar images by the number of selected images with the same pseudo-label. Third, we select the most informative image for annotator labeling, which has the top-1 high re-weighted uncertainty score. Then we select data iteratively following the above steps until reaching the budget limited of the one batch of images. In the end, we add the batch of images as the object detector's training data. We do four experiments to validate that data selection in our method is more efficient and robust. Besides, we organize the recommendation usage of each method in different environments. Finally, we can accelerate the surveillance system build-up time and the data collection through our method. In most environments, we can only use the 30% data to achieve a competitive model 90% performance with the entire dataset.
</div>

</td>
</tr>

<tr id="tr-wenyenchang2020_360" style="background-color: #ffffd0">
<td class="col-md-3"><a href='https://arxiv.org/abs/2008.12775' target='_blank'><img src="images/publications/abs_wenyenchang2020_360.png" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em>360-Indoor: Towards Learning Real-World Objects in 360deg Indoor Equirectangular Images</em><br>
    Shih-Han Chou, Cheng Sun, <strong>Wen-Yen Chang</strong>, Wan-Ting Hsu, Min Sun, Jianlong Fu<br>
    WACV 2020<br>
    [2] 
[<a href='javascript:;'
    onclick='$("#abs_wenyenchang2020_360").toggle()'>abs</a>] [<a href='https://arxiv.org/pdf/1910.01712' target='_blank'>pdf</a>] <br>
    
<div id="abs_wenyenchang2020_360" style="text-align: justify; display: none" markdown="1">
While there are several widely used object detection datasets, current computer vision algorithms are still limited in conventional images. Such images narrow our vision in a restricted region. On the other hand, 360deg images provide a thorough sight. In this paper, our goal is to provide a standard dataset to facilitate the vision and machine learning communities in 360deg domain. To facilitate the research, we present a real-world 360deg panoramic object detection dataset, 360-Indoor, which is a new benchmark for visual object detection and class recognition in 360deg indoor images. It is achieved by gathering images of complex indoor scenes containing common objects and the intensive annotated bounding field-of-view. In addition, 360-Indoor has several distinct properties:(1) the largest category number (37 labels in total).(2) the most complete annotations on average (27 bounding boxes per image). The selected 37 objects are all common in indoor scene. With around 3k images and 90k labels in total, 360-Indoor achieves the largest dataset for detection in 360deg images. In the end, extensive experiments on the state-of-the-art methods for both classification and detection are provided. We will release this dataset in the near future.
</div>

</td>
</tr>


</table>
<h2>2019</h2>
<table class="table table-hover">

<tr id="tr-wenyen2019bias_aware" style="background-color: #ffffd0">
<td class="col-md-3"><a href='https://arxiv.org/pdf/1911.07574' target='_blank'><img src="images/publications/bias_aware_heapify_policy.PNG" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em>Bias-Aware Heapified Policy for Active Learning</em><br>
    <strong>Wen-Yen Chang</strong>, Wen-Huan Chiang, Shao-Hao Lu, Tingfan Wu, Min Sun<br>
    CVGIP 2019  <br>
    [3] 
<!-- [<a href='javascript:;'
    onclick='$("#abs_wenyen2019bias_aware").toggle()'>abs</a>] [<a href='https://arxiv.org/pdf/1911.07574.pdf' target='_blank'>pdf</a>]  [<a href='https://github.com/facebookresearch/dcem' target='_blank'>code</a>] <br> -->
[<a href='javascript:;'
    onclick='$("#abs_wenyen2019bias_aware").toggle()'>abs</a>] [<a href='https://arxiv.org/pdf/1911.07574.pdf' target='_blank'>pdf</a>] <br>
<div id="abs_wenyen2019bias_aware" style="text-align: justify; display: none" markdown="1">
The data efficiency of learning-based algorithms is more and more important since high-quality and clean data is expensive as well as hard to collect. In order to achieve high model performance with the least number of samples, active learning is a technique that queries the most important subset of data from the original dataset. In active learning domain, one of the mainstream research is the heuristic uncertainty-based method which is useful for the learning-based system. Recently, a few works propose to apply policy reinforcement learning (PRL) for querying important data. It seems more general than heuristic uncertainty-based method owing that PRL method depends on data feature which is reliable than human prior. However, there have two problems-sample inefficiency of policy learning and overconfidence, when applying PRL on active learning. To be more precise, sample inefficiency of policy learning occurs when sampling within a large action space, in the meanwhile, class imbalance can lead to the overconfidence. In this paper, we propose a bias-aware policy network called Heapified Active Learning (HAL), which prevents overconfidence, and improves sample efficiency of policy learning by heapified structure without ignoring global inforamtion (overview of the whole unlabeled set). In our experiment, HAL outperforms other baseline methods on MNIST dataset and duplicated MNIST. Last but not least, we investigate the generalization of the HAL policy learned on MNIST dataset by directly applying it on MNIST-M. We show that the agent can generalize and outperform directly-learned policy under constrained labeled sets.
</div>

</td>
</tr>

</table>
<h2>2018</h2>
<table class="table table-hover">

<tr id="tr-wenyen2018LMP" style="background-color: #ffffd0">
<td class="col-md-3"><a href='https://openaccess.thecvf.com/content_ECCV_2018/papers/Yu-Ting_Chen_Leveraging_Motion_Priors_ECCV_2018_paper.pdf' target='_blank'><img src="images/publications/wenyen2018LMP.png" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em>Leveraging Motion Priors in Videos for Improving Human Segmentation</em><br>
    Yu-Ting Chen, <strong>Wen-Yen Chang</strong>, Hai-Lun Lu, Tingfan Wu, Min Sun<br>
    ECCV 2018<br>
    [4] 
    [<a href='javascript:;'
    onclick='$("#abs_wenyen2018LMP").toggle()'>abs</a>][<a href='https://openaccess.thecvf.com/content_ECCV_2018/papers/Yu-Ting_Chen_Leveraging_Motion_Priors_ECCV_2018_paper.pdf' target='_blank'>pdf</a>]  [<a href='https://github.com/Jwy-Leo/LMP' target='_blank'>code</a>] <br>
<div id="abs_wenyen2018LMP" style="text-align: justify; display: none" markdown="1">
    Despite many advances in deep-learning based semantic segmentation, performance drop due to distribution mismatch is often encountered in the real world. Recently, a few domain adaptation and active learning approaches have been proposed to mitigate the performance drop. However, very little attention has been made toward leveraging information in videos which are naturally captured in most camera systems. In this work, we propose to leverage" motion prior" in videos for improving human segmentation in a weakly-supervised active learning setting. By extracting motion information using optical flow in videos, we can extract candidate foreground motion segments (referred to as motion prior) potentially corresponding to human segments. We propose to learn a memory-network-based policy model to select strong candidate segments (referred to as strong motion prior) through reinforcement learning. The selected segments have high precision and are directly used to finetune the model. In a newly collected surveillance camera dataset and a publicly available UrbanStreet dataset, our proposed method improves the performance of human segmentation across multiple scenes and modalities (ie, RGB to Infrared (IR)). Last but not least, our method is empirically complementary to existing domain adaptation approaches such that additional performance gain is achieved by combining our weakly-supervised active learning approach with domain adaptation approaches.
</div>
</td>
</tr>
</table>

<!-- Selected Projects -->
## <i class="fa fa-chevron-right"></i>Selected Projects
<h2>2018</h2>
<table class="table table-hover">
<tr id="tr-wenyen2018Cinema-graph" style="background-color: #ffffd0">
<td class="col-md-3"><a href='https://drive.google.com/file/d/1G7527qf9yZ0EfuxdlIoT_tPR1inlK6-_/view?usp=sharing' target='_blank'><img src="images/publications/wenyen2018_cinemagraph.gif" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em>JoRaLe Auto-Cinemagraph</em><br>
    Tsun-Hsuan Wang, <strong>Wen-Yen Chang</strong>, Chun-Hung Chao<br>
    [5] 
    [<a href='javascript:;'
    onclick='$("#abs_wenyen2018Cinema-graph").toggle()'>abs</a>][<a href='https://drive.google.com/file/d/1G7527qf9yZ0EfuxdlIoT_tPR1inlK6-_/view?usp=sharing' target='_blank'>pdf</a>]  [<a href='https://docs.google.com/presentation/d/1FKrzxopTsm-ZrAY0hcuPicTJPVEiZE7PCiy58d4KPT8/edit##slide=id.p' target='_blank'>slide</a>] <br>
<div id="abs_wenyen2018Cinema-graph" style="text-align: justify; display: none" markdown="1">
   Automatically create Cinema-graph from video, we apply few-shot labeling tool and use computer vision techniques.
</div>
</td>
</tr>

<tr id="tr-wenyen2018Cinema-graph" style="background-color: #ffffd0">
<td class="col-md-3"><a href='https://drive.google.com/file/d/1fVjTz5ySJtjuBqj3wsLjHbZE3wXjzfLN/view' target='_blank'><img src="images/publications/wenyen2018_taxi-bidding_simulation.PNG" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em>Taxi-bidding simulation(auction rule)</em><br>
    Meng-Li Shih, <strong>Wen-Yen Chang</strong>, Chun-Hung Chao<br>
    [6] 
    [<a href='https://drive.google.com/file/d/1fVjTz5ySJtjuBqj3wsLjHbZE3wXjzfLN/view' target='_blank'>pdf</a>]  [<a href='https://www.youtube.com/watch?v=OAhS7IFavqk' target='_blank'>video</a>] <br>
</td>
</tr>
</table>

<h2>2017</h2>
<table class="table table-hover">
<tr id="tr-wenyen2017ParkingLotAPP" style="background-color: #ffffd0">
<td class="col-md-3"><a href='https://drive.google.com/file/d/0B_Idv-dGumOZUzJYcUlZT3BFdFk/view?usp=sharing' target='_blank'><img src="images/publications/wenyen_bachelor_independent_study.gif" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em>Car tracking and finding service in parkinglot system</em><br>
    <strong>Wen-Yen Chang</strong><br>
    Undergraudated project <br>
    [7] 
    [<a href='javascript:;'
    onclick='$("#abs_wenyen2017ParkingLotAPP").toggle()'>abs</a>][<a href='https://drive.google.com/file/d/0B_Idv-dGumOZUzJYcUlZT3BFdFk/view?usp=sharing' target='_blank'>pdf</a>]  [<a href='https://www.youtube.com/watch?v=T1_LeuHiyTA&t=1s' target='_blank'>video</a>] <br>
<div id="abs_wenyen2017ParkingLotAPP" style="text-align: justify; display: none" markdown="1">
  • Getting more reliable tracking results, I use image motion to enhance image-based object detection model.<br>
  • Eliminating GPS localization error, I use trajectory information to reduce the white noise by Kalman Filter.<br>
  • Checking parking location for finding, I fuse GPS and image localization information by driving behavior matching.<br>
</div>
</td>
</tr>

<tr id="tr-wenyen2017HomeCareAPP" style="background-color: #ffffd0">
<td class="col-md-3"><a href='https://drive.google.com/file/d/0B_Idv-dGumOZRjlLR3JLeTdGNnM/view' target='_blank'><img src="images/publications/Wenyen_Homecare_APP.PNG" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em>Home Care App.</em><br>
    <strong>Wen-Yen Chang</strong>, Kang-Yu Liou, Kuan-Yeh Wu, Ya-Chu Chang, Po-Hsien Wang, Ting-Ying Wang<br>
    [8] 
    [<a href='javascript:;'
    onclick='$("#abs_wenyen2017HomeCareAPP").toggle()'>abs</a>][<a href='https://drive.google.com/file/d/0B_Idv-dGumOZRjlLR3JLeTdGNnM/view' target='_blank'>pdf</a>]<br>
<div id="abs_wenyen2017HomeCareAPP" style="text-align: justify; display: none" markdown="1">
  A android application to recognize the number on blood-pressure machine. The recognition system need to deal with multiple material display. 
</div>
</td>
</tr>
</table>


<!-- Skills -->
## <i class="fa fa-chevron-right"></i> Skills
<table class="table table-hover">
<tr>
  <td class='col-md-2'>Programming Languages</td>
  <td>
C, C++, C#, Java, Python, Matlab, Assembly
  </td>
</tr>
<tr>
  <td class='col-md-2'>Deep Learning Frameworks</td>
  <td>
Caffe, Tensorflow, Pytorch
  </td>
</tr>
<tr>
  <td class='col-md-2'>Web Tools</td>
  <td>
html, css, javascript, ruby
  </td>
</tr>
<tr>
  <td class='col-md-2'>UI Tools</td>
  <td>
MFC, QT
  </td>
</tr>

<tr>
  <td class='col-md-2'>Hardware/Firmware (wo/ UNIX OS)</td>
  <td>
8051, Arduino, Nu-LB-NUC140
  </td>
</tr>

<tr>
  <td class='col-md-2'>Hardware/Firmware (w/ UNIX OS)</td>
  <td>
Raspberry-Pi zero, TX2(ROS)
  </td>
</tr>

<tr>
  <td class='col-md-2'>Hardware descript language</td>
  <td>
Verilog
  </td>
</tr>
<tr>
  <td class='col-md-2'>Misc.</td>
  <td>
OpenCV, OpenGL, Github, Docker, Vim, Linux, LATEX
  </td>
</tr>
</table>
